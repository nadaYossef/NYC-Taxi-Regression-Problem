{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "piIm5rzG-6xo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/NYC.csv\")\n",
        "df.head(8)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 497
        },
        "id": "n7MfbRu__H9X",
        "outputId": "90ebbe8a-6fc4-49dd-e97f-f092cbac7da4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ParserError",
          "evalue": "Error tokenizing data. C error: Expected 11 fields in line 15249, saw 15\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-426589693.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/NYC.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    624\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 626\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1921\u001b[0m                     \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1922\u001b[0m                     \u001b[0mcol_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1923\u001b[0;31m                 \u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1924\u001b[0m                     \u001b[0mnrows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1925\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlow_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m                 \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_low_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m                 \u001b[0;31m# destructive to chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_concatenate_chunks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 11 fields in line 15249, saw 15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "KBtQTmOZACP0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "8fSK2SNNAAP3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observations:\n",
        "1. missing values found in some columns\n",
        "2. datetime columns are of object types"
      ],
      "metadata": {
        "id": "zx_uCUiLAHZk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "null_per = df.isnull().sum()/df.shape[0]*100\n",
        "null_per"
      ],
      "metadata": {
        "id": "f21DY2H1AXgQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.heatmap(df.isnull(), cbar=False, cmap='viridis', yticklabels=False)"
      ],
      "metadata": {
        "id": "Ute63Y7UC7v-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def impute_nulls(col):\n",
        "  df[col].fillna(df[col].mean(), inplace=True)\n",
        "\n",
        "# Convert datetime columns to datetime objects with error handling\n",
        "df['pickup_datetime'] = pd.to_datetime(df['pickup_datetime'])\n",
        "df['dropoff_datetime'] = pd.to_datetime(df['dropoff_datetime'])\n",
        "\n",
        "# Impute nulls\n",
        "impute_nulls(\"dropoff_longitude\")\n",
        "impute_nulls(\"dropoff_latitude\")\n",
        "\n",
        "df['store_and_fwd_flag'].fillna(df['store_and_fwd_flag'].mode()[0], inplace=True)\n",
        "\n",
        "impute_nulls(\"trip_duration\")\n",
        "\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "EiRhhOgQA6qp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe()"
      ],
      "metadata": {
        "id": "H4b9DMsPCMwf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observations:\n",
        "1. minimum passenger count is 0 which is quite impossible\n",
        "2. The maximum duration is approximately 3.5 million seconds in trip duration\n",
        "3. Geospatial Columns have extremely wide ranges"
      ],
      "metadata": {
        "id": "1B1QDiGKCq1_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sum(df.duplicated())"
      ],
      "metadata": {
        "id": "gN8Se3ICKKIS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_cols = df.select_dtypes(include=np.number).columns\n",
        "outlier_summary = {}\n",
        "\n",
        "for col in num_cols:\n",
        "    Q1 = df[col].quantile(0.25)\n",
        "    Q3 = df[col].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "    outliers = ((df[col] < lower_bound) | (df[col] > upper_bound)).sum()\n",
        "    outlier_summary[col] = {\n",
        "        'lower_bound': lower_bound,\n",
        "        'upper_bound': upper_bound,\n",
        "        'outlier_count': outliers,\n",
        "        'outlier_percentage': 100 * outliers / len(df)\n",
        "    }\n",
        "\n",
        "outlier_df = pd.DataFrame(outlier_summary).T\n",
        "outlier_df"
      ],
      "metadata": {
        "id": "upZd-TeNINcG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.vendor_id.value_counts()"
      ],
      "metadata": {
        "id": "1zRGpUAAM_aV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.passenger_count.value_counts()"
      ],
      "metadata": {
        "id": "cs8hD8ufNhO6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "invalid_passenger_counts = [0, 7, 8, 9]\n",
        "\n",
        "df = df[~df['passenger_count'].isin(invalid_passenger_counts)]\n",
        "\n",
        "print(df['passenger_count'].value_counts())\n",
        "print(df.shape)"
      ],
      "metadata": {
        "id": "QWeOsrgRPYoy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lat_min, lat_max = 40.49, 40.92\n",
        "lon_min, lon_max = -74.27, -73.68\n",
        "\n",
        "pickup_outliers = df[\n",
        "    (df['pickup_latitude'] < lat_min) | (df['pickup_latitude'] > lat_max) |\n",
        "    (df['pickup_longitude'] < lon_min) | (df['pickup_longitude'] > lon_max)\n",
        "]\n",
        "\n",
        "print(\"Pickup outliers:\", pickup_outliers.shape)\n",
        "pickup_outliers.head()\n"
      ],
      "metadata": {
        "id": "Z46NhdAeOu2R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dropoff_outliers = df[\n",
        "    (df['dropoff_latitude'] < lat_min) | (df['dropoff_latitude'] > lat_max) |\n",
        "    (df['dropoff_longitude'] < lon_min) | (df['dropoff_longitude'] > lon_max)\n",
        "]\n",
        "\n",
        "print(\"Dropoff outliers:\", dropoff_outliers.shape)\n",
        "dropoff_outliers.head()"
      ],
      "metadata": {
        "id": "bHTTm_8BO6f6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Haversine function (km) - returns a NumPy array\n",
        "def haversine(lon1, lat1, lon2, lat2):\n",
        "    R = 6371  # Earth radius in km\n",
        "    lon1, lat1, lon2, lat2 = map(np.radians, [lon1, lat1, lon2, lat2])\n",
        "    dlon = lon2 - lon1\n",
        "    dlat = lat2 - lat1\n",
        "    a = np.sin(dlat / 2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2)**2\n",
        "    return R * 2 * np.arcsin(np.sqrt(a))\n",
        "\n",
        "trip_distances = haversine(\n",
        "    df['pickup_longitude'], df['pickup_latitude'],\n",
        "    df['dropoff_longitude'], df['dropoff_latitude']\n",
        ")\n",
        "\n",
        "coord_outlier = (\n",
        "    (~df['pickup_latitude'].between(lat_min, lat_max) |\n",
        "     ~df['pickup_longitude'].between(lon_min, lon_max) |\n",
        "     ~df['dropoff_latitude'].between(lat_min, lat_max) |\n",
        "     ~df['dropoff_longitude'].between(lon_min, lon_max))\n",
        ")\n",
        "\n",
        "df = df[~(coord_outlier & (trip_distances > 150))]\n",
        "\n",
        "Q1 = df['trip_duration'].quantile(0.25)\n",
        "Q3 = df['trip_duration'].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "lower_bound = Q1 - 1.5 * IQR\n",
        "upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "df = df[(df['trip_duration'] >= lower_bound) & (df['trip_duration'] <= upper_bound)]\n",
        "\n",
        "print(df.shape)"
      ],
      "metadata": {
        "id": "2ami8HmyQvdA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.store_and_fwd_flag.value_counts()"
      ],
      "metadata": {
        "id": "fwGT8aWlQ5kG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(5,5))\n",
        "df['store_and_fwd_flag'].value_counts().plot(\n",
        "    kind='pie',\n",
        "    autopct='%1.1f%%',\n",
        "    startangle=90,\n",
        "    colors=['skyblue', 'salmon'],\n",
        "    labels=['N', 'Y']\n",
        ")\n",
        "plt.ylabel('')\n",
        "plt.title('Store and Forward Flag Distribution')"
      ],
      "metadata": {
        "id": "v9dg6Y-zRRvL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['store_and_fwd_flag'] = df['store_and_fwd_flag'].map({'N': 0, 'Y': 1})\n",
        "df.tail()"
      ],
      "metadata": {
        "id": "_BLvWMANSTBI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df['pickup_datetime'].dtype)\n",
        "\n",
        "# Check date range\n",
        "print(df['pickup_datetime'].min(), df['pickup_datetime'].max())\n",
        "\n",
        "# Check for negative trip durations\n",
        "print((df['dropoff_datetime'] < df['pickup_datetime']).sum())\n",
        "\n",
        "# Check duplicates\n",
        "print(df.duplicated(subset=['pickup_datetime', 'dropoff_datetime']).sum())"
      ],
      "metadata": {
        "id": "z7uB3rNNTFM1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.drop_duplicates(subset=['pickup_datetime', 'dropoff_datetime'])"
      ],
      "metadata": {
        "id": "L36sFAFjUAum"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12,6))\n",
        "df.set_index('pickup_datetime').resample('D').size().plot()\n",
        "plt.title('Daily Trip Counts')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Number of Trips')"
      ],
      "metadata": {
        "id": "VXBJsgQLS5wY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Daily trip counts\n",
        "daily_counts = df.set_index('pickup_datetime').resample('D').size()\n",
        "\n",
        "# Find lowest day\n",
        "min_day = daily_counts.idxmin()\n",
        "print(\"Lowest trip day:\", min_day, \"with\", daily_counts.min(), \"trips\")\n",
        "\n",
        "# Look at hourly distribution for that day\n",
        "hourly_counts = df[df['pickup_datetime'].dt.date == min_day.date()] \\\n",
        "    .set_index('pickup_datetime') \\\n",
        "    .resample('H').size()\n",
        "print(hourly_counts)"
      ],
      "metadata": {
        "id": "etAH46HQUVvi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "January 23, 2016 was the day of the big NYC blizzard (“Winter Storm Jonas”).\n",
        "Almost all public transport and taxi service shut down in the afternoon, so the trip counts crashed to nearly zero after 3 PM.\n",
        "\n",
        "That’s why you see:\n",
        "\n",
        "Normal trip volume until about 2 PM,\n",
        "\n",
        "Then a steep drop,\n",
        "\n",
        "Zero trips in evening/night hours."
      ],
      "metadata": {
        "id": "n7qLpCjmUr2r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mask = df['pickup_datetime'].dt.normalize() != pd.to_datetime('2016-01-23')\n",
        "df = df[mask]\n",
        "mask_dropoff = df['dropoff_datetime'].dt.normalize() != pd.to_datetime('2016-01-23')\n",
        "df = df[mask_dropoff]"
      ],
      "metadata": {
        "id": "8A3Quy4rXkI_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[df['pickup_datetime'].dt.normalize() == pd.to_datetime('2016-01-23')]"
      ],
      "metadata": {
        "id": "lt7nh2-vXh8Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[df['dropoff_datetime'].dt.normalize() == pd.to_datetime('2016-01-23')]"
      ],
      "metadata": {
        "id": "nh44x27QYUog"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "IvcS6v5QYfzj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pickup features\n",
        "df['pickup_year'] = df['pickup_datetime'].dt.year\n",
        "df['pickup_month'] = df['pickup_datetime'].dt.month\n",
        "df['pickup_day'] = df['pickup_datetime'].dt.day\n",
        "df['pickup_dayofweek'] = df['pickup_datetime'].dt.dayofweek\n",
        "df['pickup_hour'] = df['pickup_datetime'].dt.hour\n",
        "df['pickup_weekofyear'] = df['pickup_datetime'].dt.isocalendar().week.astype(int)\n",
        "df['is_weekend'] = (df['pickup_datetime'].dt.dayofweek >= 5).astype(int)\n",
        "\n",
        "# Dropoff features\n",
        "df['dropoff_year'] = df['dropoff_datetime'].dt.year\n",
        "df['dropoff_month'] = df['dropoff_datetime'].dt.month\n",
        "df['dropoff_day'] = df['dropoff_datetime'].dt.day\n",
        "df['dropoff_dayofweek'] = df['dropoff_datetime'].dt.dayofweek\n",
        "df['dropoff_hour'] = df['dropoff_datetime'].dt.hour\n",
        "df['dropoff_weekofyear'] = df['dropoff_datetime'].dt.isocalendar().week.astype(int)\n",
        "df['dropoff_is_weekend'] = (df['dropoff_datetime'].dt.dayofweek >= 5).astype(int)\n",
        "\n",
        "df = df.drop(['pickup_datetime', 'dropoff_datetime'], axis=1)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "Xp7SkV7yZZ0I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "\n",
        "df_model = df.drop(columns=['id']).copy()\n",
        "\n",
        "# Log-transform trip_duration to reduce skew\n",
        "df_model['trip_duration'] = np.log1p(df_model['trip_duration'])\n",
        "\n",
        "# Scale continuous features (z-score standardization)\n",
        "scaler = StandardScaler()\n",
        "df_scaled = pd.DataFrame(\n",
        "    scaler.fit_transform(df_model),\n",
        "    columns=df_model.columns,\n",
        "    index=df_model.index\n",
        ")\n",
        "\n",
        "# Normalize to [0, 1]\n",
        "normalizer = MinMaxScaler()\n",
        "df_normalized = pd.DataFrame(\n",
        "    normalizer.fit_transform(df_scaled),\n",
        "    columns=df_scaled.columns,\n",
        "    index=df_scaled.index\n",
        ")\n",
        "\n",
        "df = df_normalized\n",
        "\n",
        "print(\"Final df shape:\", df.shape)\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "rVXn3wk9cIWF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "\n",
        "\n",
        "X = df.drop(columns=['trip_duration'])  # drop target + id\n",
        "y = df['trip_duration']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "def evaluate_model(model, name):\n",
        "    model.fit(X_train, y_train)\n",
        "    y_train_pred = model.predict(X_train)\n",
        "    y_test_pred = model.predict(X_test)\n",
        "\n",
        "    results = {\n",
        "        'Model': name,\n",
        "        'Train RMSE': np.sqrt(mean_squared_error(y_train, y_train_pred)),\n",
        "        'Test RMSE': np.sqrt(mean_squared_error(y_test, y_test_pred)),\n",
        "        'Train R²': r2_score(y_train, y_train_pred),\n",
        "        'Test R²': r2_score(y_test, y_test_pred)\n",
        "    }\n",
        "    return results\n",
        "\n",
        "# Example usage\n",
        "from sklearn.linear_model import LinearRegression, Ridge\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "results = []\n",
        "results.append(evaluate_model(LinearRegression(), \"Linear Regression\"))\n",
        "results.append(evaluate_model(make_pipeline(PolynomialFeatures(degree=2), LinearRegression()), \"Polynomial Regression (deg=2)\"))\n",
        "results.append(evaluate_model(Ridge(alpha=1.0), \"Ridge Regression\"))\n",
        "\n",
        "pd.DataFrame(results)\n"
      ],
      "metadata": {
        "id": "3zWdGxnTjR9a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import Ridge, Lasso\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Create a pipeline for Ridge with Polynomial Features\n",
        "ridge_poly_pipeline = Pipeline([\n",
        "    ('poly', PolynomialFeatures(degree=2)),\n",
        "    ('ridge', Ridge())\n",
        "])\n",
        "param_grid_ridge = {'ridge__alpha': [0.1, 1.0, 10.0, 100.0]}\n",
        "ridge_cv = GridSearchCV(ridge_poly_pipeline, param_grid_ridge, cv=5, scoring='r2', n_jobs=-1)\n",
        "ridge_cv.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best Ridge Polynomial parameters:\", ridge_cv.best_params_)\n",
        "print(\"Optimized Ridge Polynomial Test R² score:\", ridge_cv.best_estimator_.score(X_test, y_test))\n",
        "print(\"-\" * 50)"
      ],
      "metadata": {
        "id": "JDgl-5OlIFh0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a pipeline for Lasso with Polynomial Features\n",
        "lasso_poly_pipeline = Pipeline([\n",
        "    ('poly', PolynomialFeatures(degree=2)),\n",
        "    ('lasso', Lasso())\n",
        "])\n",
        "param_grid_lasso = {'lasso__alpha': [0.001, 0.01, 0.1, 1.0]}\n",
        "lasso_cv = GridSearchCV(lasso_poly_pipeline, param_grid_lasso, cv=5, scoring='r2', n_jobs=-1)\n",
        "lasso_cv.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best Lasso Polynomial parameters:\", lasso_cv.best_params_)\n",
        "print(\"Optimized Lasso Polynomial Test R² score:\", lasso_cv.best_estimator_.score(X_test, y_test))"
      ],
      "metadata": {
        "id": "NFk2_Q_oJh-n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "import xgboost as xgb\n",
        "\n",
        "results = []\n",
        "results.append(evaluate_model(RandomForestRegressor(n_estimators=100, random_state=42), \"Random Forest Regressor\"))\n",
        "results.append(evaluate_model(GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42), \"Gradient Boosting Regressor\"))\n",
        "results.append(evaluate_model(xgb.XGBRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42), \"XGBoost Regressor\"))\n",
        "\n",
        "new_results_df = pd.DataFrame(results)\n",
        "print(new_results_df)"
      ],
      "metadata": {
        "id": "L2K0mPw5_qjt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "# Random Forest\n",
        "rf = RandomForestRegressor(random_state=42)\n",
        "param_dist_rf = {\n",
        "    \"n_estimators\": [100, 300, 500],\n",
        "    \"max_depth\": [None, 10, 20, 30],\n",
        "    \"min_samples_split\": [2, 5, 10],\n",
        "    \"min_samples_leaf\": [1, 2, 4],\n",
        "    \"max_features\": [\"auto\", \"sqrt\", 0.5]\n",
        "}\n",
        "rf_cv = RandomizedSearchCV(rf, param_dist_rf, cv=3, n_iter=20,\n",
        "                           scoring=\"r2\", n_jobs=-1, random_state=42)\n",
        "rf_cv.fit(X_train, y_train)\n",
        "print(\"Best RF params:\", rf_cv.best_params_)\n",
        "print(\"RF Test R²:\", rf_cv.best_estimator_.score(X_test, y_test))\n",
        "\n",
        "# XGBoost\n",
        "xgb_reg = xgb.XGBRegressor(random_state=42, objective=\"reg:squarederror\")\n",
        "param_dist_xgb = {\n",
        "    \"n_estimators\": [100, 300, 500],\n",
        "    \"max_depth\": [3, 5, 7],\n",
        "    \"learning_rate\": [0.01, 0.05, 0.1],\n",
        "    \"subsample\": [0.6, 0.8, 1.0],\n",
        "    \"colsample_bytree\": [0.6, 0.8, 1.0],\n",
        "    \"reg_lambda\": [0.1, 1, 10]\n",
        "}\n",
        "xgb_cv = RandomizedSearchCV(xgb_reg, param_dist_xgb, cv=3, n_iter=20,\n",
        "                            scoring=\"r2\", n_jobs=-1, random_state=42)\n",
        "xgb_cv.fit(X_train, y_train)\n",
        "print(\"Best XGB params:\", xgb_cv.best_params_)\n",
        "print(\"XGB Test R²:\", xgb_cv.best_estimator_.score(X_test, y_test))\n"
      ],
      "metadata": {
        "id": "-NHMKzkuubyZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}